# ğŸ”§ Fine-Tuning Gemma 7B with LoRA & TRL

This project demonstrates how to fine-tune **Googleâ€™s Gemma-7B** language model using **LoRA (Low-Rank Adaptation)** and **Hugging Faceâ€™s TRL (SFTTrainer)**. It supports **4-bit quantization** for efficient GPU memory usage and includes an interactive CLI chatbot built from the fine-tuned model.

---

## ğŸ“Œ Key Features

- âš¡ 4-bit quantized loading with `bitsandbytes`
- ğŸ” LoRA-based fine-tuning with `peft`
- ğŸ“š Supervised Fine-Tuning with `trl`'s `SFTTrainer`
- ğŸ§  Prompt formatting using paired `input`/`output` JSON
- ğŸ’¬ Inference-ready merged model with interactive CLI
- ğŸ§° Fully GPU-accelerated with `transformers`, `datasets`, and `accelerate`

---

## ğŸ§  Requirements

Install all dependencies with:

```bash
pip install -r requirements.txt
